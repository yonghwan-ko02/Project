#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ollama GPU ì‚¬ìš© í™•ì¸ ë° ìµœì í™” ìŠ¤í¬ë¦½íŠ¸
"""

import urllib.request
import json
import sys
import subprocess

# Windows ì½˜ì†” UTF-8 ì¸ì½”ë”© ì„¤ì •
if sys.platform == 'win32':
    import codecs
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer, 'strict')

def check_gpu_status():
    """nvidia-smië¡œ GPU ìƒíƒœ í™•ì¸"""
    print("ğŸ” GPU ìƒíƒœ í™•ì¸ ì¤‘...\n")
    
    try:
        result = subprocess.run(
            ['nvidia-smi', '--query-gpu=name,memory.total,memory.used,memory.free,utilization.gpu', '--format=csv,noheader'],
            capture_output=True,
            text=True,
            timeout=5
        )
        
        if result.returncode == 0:
            info = result.stdout.strip().split(', ')
            print(f"ğŸ“Š GPU ì •ë³´:")
            print(f"  ì´ë¦„: {info[0]}")
            print(f"  ì´ VRAM: {info[1]}")
            print(f"  ì‚¬ìš© ì¤‘: {info[2]}")
            print(f"  ì—¬ìœ : {info[3]}")
            print(f"  ì‚¬ìš©ë¥ : {info[4]}")
            
            # VRAM ì—¬ìœ  ê³µê°„ í™•ì¸
            free_mb = int(info[3].replace(' MiB', ''))
            if free_mb < 1000:
                print(f"\nâš ï¸ VRAM ì—¬ìœ  ê³µê°„ì´ ë¶€ì¡±í•©ë‹ˆë‹¤ ({free_mb}MB)")
                print("  ë‹¤ë¥¸ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•˜ê±°ë‚˜ Ollamaë¥¼ ì¬ì‹œì‘í•˜ì„¸ìš”.")
                return False
            else:
                print(f"\nâœ… VRAM ì—¬ìœ  ê³µê°„ ì¶©ë¶„: {free_mb}MB")
                return True
        else:
            print("âš ï¸ nvidia-smi ì‹¤í–‰ ì‹¤íŒ¨")
            return False
            
    except FileNotFoundError:
        print("âŒ nvidia-smië¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. NVIDIA ë“œë¼ì´ë²„ê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        return False
    except Exception as e:
        print(f"âŒ GPU ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
        return False

def check_ollama_gpu():
    """Ollamaê°€ GPUë¥¼ ì‚¬ìš©í•˜ëŠ”ì§€ í™•ì¸"""
    print("\nğŸ” Ollama GPU ì‚¬ìš© í™•ì¸ ì¤‘...\n")
    
    try:
        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ìƒì„± ìš”ì²­
        url = "http://localhost:11434/api/generate"
        data = json.dumps({
            "model": "llama3.1:8b-instruct-q4_K_M",
            "prompt": "Hello",
            "stream": False
        }).encode('utf-8')
        
        req = urllib.request.Request(
            url,
            data=data,
            headers={'Content-Type': 'application/json'}
        )
        
        print("ğŸ“ í…ŒìŠ¤íŠ¸ ìƒì„± ìš”ì²­ ì¤‘...")
        import time
        start = time.time()
        
        with urllib.request.urlopen(req, timeout=30) as response:
            result = json.loads(response.read().decode())
            elapsed = time.time() - start
            
            print(f"âœ… ì‘ë‹µ ì‹œê°„: {elapsed:.2f}ì´ˆ")
            
            # GPU ì‚¬ìš© ì—¬ë¶€ íŒë‹¨ (GPU ì‚¬ìš© ì‹œ í›¨ì”¬ ë¹ ë¦„)
            if elapsed < 5:
                print("ğŸ® GPU ê°€ì†ì´ í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤!")
                return True
            elif elapsed < 15:
                print("âš ï¸ GPUë¥¼ ì‚¬ìš©í•˜ê³  ìˆì§€ë§Œ ëŠë¦½ë‹ˆë‹¤. VRAM ë¶€ì¡± ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤.")
                return True
            else:
                print("âŒ CPUë¡œ ì‹¤í–‰ ì¤‘ì¸ ê²ƒ ê°™ìŠµë‹ˆë‹¤. ë§¤ìš° ëŠë¦½ë‹ˆë‹¤.")
                return False
                
    except urllib.error.URLError:
        print("âŒ Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return False
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def get_optimization_tips():
    """ìµœì í™” íŒ ì œê³µ"""
    print("\n" + "="*60)
    print("ğŸ’¡ ì„±ëŠ¥ ìµœì í™” íŒ")
    print("="*60)
    print("""
1. VRAM í™•ë³´:
   - Chrome, Slack ë“± GPU ì‚¬ìš© í”„ë¡œê·¸ë¨ ì¢…ë£Œ
   - ë¶ˆí•„ìš”í•œ ë°±ê·¸ë¼ìš´ë“œ í”„ë¡œê·¸ë¨ ì¢…ë£Œ

2. Ollama ì¬ì‹œì‘:
   - ì‘ì—… ê´€ë¦¬ìì—ì„œ Ollama í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ
   - Ollama ë‹¤ì‹œ ì‹œì‘

3. ëª¨ë¸ ì–¸ë¡œë“œ:
   - ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ ì–¸ë¡œë“œí•˜ì—¬ VRAM í™•ë³´

4. ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš©:
   - llama3.1:3b (ë” ì‘ê³  ë¹ ë¦„)
   - ëŒ€ì‹  í’ˆì§ˆì€ ì•½ê°„ ë‚®ì•„ì§ˆ ìˆ˜ ìˆìŒ

5. í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (ì´ë¯¸ ì ìš©ë˜ì–´ ìˆì„ ê°€ëŠ¥ì„± ë†’ìŒ):
   - OLLAMA_GPU_LAYERS=999 (ëª¨ë“  ë ˆì´ì–´ë¥¼ GPUì—)
   - OLLAMA_NUM_GPU=1
""")

def main():
    print("ğŸš€ Ollama GPU ìµœì í™” ë„êµ¬")
    print("="*60)
    
    # 1. GPU ìƒíƒœ í™•ì¸
    gpu_ok = check_gpu_status()
    
    # 2. Ollama GPU ì‚¬ìš© í™•ì¸
    ollama_gpu = check_ollama_gpu()
    
    # 3. ê²°ê³¼ ë° ê¶Œì¥ì‚¬í•­
    print("\n" + "="*60)
    print("ğŸ“Š ìµœì¢… ê²°ê³¼")
    print("="*60)
    
    if gpu_ok and ollama_gpu:
        print("âœ… GPU ê°€ì†ì´ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•˜ê³  ìˆìŠµë‹ˆë‹¤!")
        print("   ê²Œì„ ì‹¤í–‰ ì‹œ ë¹ ë¥¸ ì‘ë‹µì„ ê¸°ëŒ€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    elif ollama_gpu:
        print("âš ï¸ GPUë¥¼ ì‚¬ìš©í•˜ê³  ìˆì§€ë§Œ VRAMì´ ë¶€ì¡±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        print("   ë‹¤ë¥¸ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•˜ë©´ ì„±ëŠ¥ì´ í–¥ìƒë©ë‹ˆë‹¤.")
    else:
        print("âŒ GPU ê°€ì†ì´ ì œëŒ€ë¡œ ì‘ë™í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        print("   ì•„ë˜ ìµœì í™” íŒì„ ì°¸ê³ í•˜ì„¸ìš”.")
    
    # 4. ìµœì í™” íŒ
    if not (gpu_ok and ollama_gpu):
        get_optimization_tips()

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ì‚¬ìš©ìê°€ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
